{"cells":[{"metadata":{"_uuid":"a0c469df-134e-4f1e-b1e8-c513837df96c","_cell_guid":"cbd6f07f-600c-4ec0-8810-776baa20eb29","trusted":true},"cell_type":"code","source":"# Libraries\n\n#PyTorch, of course\nimport torch\nimport torch.nn as nn\n\n#We will need torchvision transforms for data augmentation\nfrom torchvision import transforms\n\n### utilities\n# tool to print a nice summary of a network, similary to keras' summary\n#from torchsummary import summary\n\n# library to do bash-like wildcard expansion\nimport glob\n\n# others\nimport numpy as np\nimport random\nfrom PIL import Image\nfrom IPython.display import display\nfrom tqdm import tqdm_notebook\n\nimport torchvision.models as models\nimport time\nimport copy\n\n# a little helper function do directly display a Tensor\ndef display_tensor(t):\n  trans = transforms.ToPILImage()\n  display(trans(t))\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%cd /kaggle/input/polytech-ds-2019/polytech-ds-2019/\n%pwd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_name = {i:category for i, category in enumerate([\"Bread\",\"Dairy products\", \"Dessert\", \"Egg\",\n                                                       \"Fried Food\",  \"Meat\", \"Noodles/Pasta\", \"Rice\",\n                                                       \"Seafood\", \"Soup\", \"Vegetable/Fruit\"])}\n\nlength_train = len(glob.glob('training'+\"/*\"))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d92f27ba-d1d6-42f3-b350-f94ea9898644","_cell_guid":"7b3bde0b-968a-4036-b918-c3864c16e3a6","trusted":true},"cell_type":"code","source":"class trainDataset(torch.utils.data.Dataset):\n    \n    def __init__(self, img_dir):\n        super().__init__()\n        self.img_dir = img_dir\n        length_train = len(glob.glob(img_dir+\"/*\"))\n        # use glob to get all image names\n        self.img_names = [x.split(\"/\")[6] for x in glob.glob(img_dir + \"/*\")]\n        self.labels = [int(img.rsplit(\"_\")[-2]) for img in self.img_names]\n\n        # PyTorch transforms\n        self.transform = transforms.Compose([transforms.RandomHorizontalFlip(p=0.5),\n                                             transforms.ColorJitter(),\n                                               transforms.Resize((224, 224)),\n                                               transforms.ToTensor(),\n                                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n                                            ])\n\n    def __len__(self):\n        return len(self.img_names)\n\n    def __getitem__(self, i):\n        return self._read_img(i)\n\n    def _read_img(self, i):\n        img = Image.open(os.path.join(self.img_dir,self.img_names[i]))\n        return self.transform(img), self.labels[i]\n\ndef display_tensor(t):\n    trans = transforms.ToPILImage()\n    display(trans(t))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training = '/kaggle/input/polytech-ds-2019/polytech-ds-2019/training'\nvalidation = '/kaggle/input/polytech-ds-2019/polytech-ds-2019/validation'\nevaluation = '/kaggle/input/polytech-ds-2019/polytech-ds-2019/kaggle_evaluation/'\ndata = [training, validation]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_tensor(t):\n    trans = transforms.ToPILImage()\n    display(trans(t))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind = 4\ntrain_set = trainDataset(training)\nval_set = trainDataset(validation)\nimg, label = train_set[ind]\nprint(\"label of {} (picture #{}) is {}\".format(train_set.img_names[ind], ind, label_name[label]))\ndisplay_tensor(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img, label = val_set[ind]\nprint(\"label of {} (picture #{}) is {}\".format(val_set.img_names[ind], ind, label_name[label]))\ndisplay_tensor(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data loader for the train dataset\ndata_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare model\ndata_dir = \"./\"\n\n# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\nmodel_name = \"resnext\"\n\n# Number of classes in the dataset\nnum_classes = 11\n\n# Batch size for training (change depending on how much memory you have)\nbatch_size = 32\n\n# Number of epochs to train for\nnum_epochs = 35\n\n# Flag for feature extracting. When False, we finetune the whole model,\n#   when True we only update the reshaped layer params\nfeature_extract = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n    since = time.time()\n\n    val_acc_history = []\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    # Get model outputs and calculate loss\n                    # Special case for inception because in training it has an auxiliary output. In train\n                    #   mode we calculate the loss by summing the final output and the auxiliary output\n                    #   but in testing we only consider the final output.\n                    if is_inception and phase == 'train':\n                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n                        outputs, aux_outputs = model(inputs)\n                        loss1 = criterion(outputs, labels)\n                        loss2 = criterion(aux_outputs, labels)\n                        loss = loss1 + 0.4*loss2\n                    else:\n                        outputs = model(inputs)\n                        loss = criterion(outputs, labels)\n\n                    _, preds = torch.max(outputs, 1)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n            if phase == 'val':\n                val_acc_history.append(epoch_acc)\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model, val_acc_history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n    # Initialize these variables which will be set in this if statement. Each of these\n    #   variables is model specific.\n    model_ft = None\n    input_size = 0\n\n    if model_name == \"resnet\":\n        \"\"\" Resnet18\n        \"\"\"\n        model_ft = models.resnet18(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n        input_size = 224\n\n    elif model_name == \"alexnet\":\n        \"\"\" Alexnet\n        \"\"\"\n        model_ft = models.alexnet(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier[6].in_features\n        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n        input_size = 224\n\n    elif model_name == \"vgg\":\n        \"\"\" VGG11_bn\n        \"\"\"\n        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier[6].in_features\n        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n        input_size = 224\n\n    elif model_name == \"squeezenet\":\n        \"\"\" Squeezenet\n        \"\"\"\n        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n        model_ft.num_classes = num_classes\n        input_size = 224\n\n    elif model_name == \"densenet\":\n        \"\"\" Densenet\n        \"\"\"\n        model_ft = models.densenet121(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier.in_features\n        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n        input_size = 224\n\n    elif model_name == \"inception\":\n        \"\"\" Inception v3\n        Be careful, expects (299,299) sized images and has auxiliary output\n        \"\"\"\n        model_ft = models.inception_v3(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        # Handle the auxilary net\n        num_ftrs = model_ft.AuxLogits.fc.in_features\n        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n        # Handle the primary net\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n        input_size = 299\n        \n    elif model_name == \"resnext\":\n        model_ft = torch.hub.load('pytorch/vision:v0.4.2', 'resnext50_32x4d', pretrained=True)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n        input_size = 224\n\n    else:\n        print(\"Invalid model name, exiting...\")\n        exit()\n\n    return model_ft, input_size\n\n# Initialize the model for this run\nmodel_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n\n# Print the model we just instantiated\nprint(model_ft)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.ColorJitter(),\n        transforms.RandomResizedCrop(input_size),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(input_size),\n        transforms.CenterCrop(input_size),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"Initializing Datasets and Dataloaders...\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimage_datasets ={'train':train_set,'val':val_set}\n\n# Create training and validation dataloaders\ndataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n#dataloader_eval = torch.utils.data.DataLoader(ev_set, batch_size=batch_size, shuffle=True, num_workers=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Send the model to GPU\n#model_ft = model_ft.to(device)\nmodel_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\nmodel_ft = model_ft.to(device)\n# Gather the parameters to be optimized/updated in this run. If we are\n#  finetuning we will be updating all parameters. However, if we are\n#  doing feature extract method, we will only update the parameters\n#  that we have just initialized, i.e. the parameters with requires_grad\n#  is True.\nparams_to_update = model_ft.parameters()\nprint(\"Params to learn:\")\nif feature_extract:\n    params_to_update = []\n    for name,param in model_ft.named_parameters():\n        if param.requires_grad == True:\n            params_to_update.append(param)\n            print(\"\\t\",name)\nelse:\n    for name,param in model_ft.named_parameters():\n        if param.requires_grad == True:\n            print(\"\\t\",name)\n\n# Observe that all parameters are being optimized\noptimizer_ft = torch.optim.SGD(params_to_update, lr=0.001, momentum=0.9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\n\n# Train and evaluate\nmodel_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class loadEvaluation(torch.utils.data.Dataset):\n    \n    def __init__(self, img_dir):\n        super().__init__()\n        self.img_dir = img_dir\n        length_train = len(glob.glob(img_dir+\"/*\"))\n        # use glob to get all image names\n        self.img_names = [x.split(\"/\")[6] for x in glob.glob(img_dir + \"/*\")]\n\n        # PyTorch transforms\n        self.transform = transforms.Compose([transforms.Resize(input_size),\n                                             transforms.CenterCrop(input_size),\n                                               transforms.ToTensor(),\n                                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n                                            ])\n\n    def __len__(self):\n        return len(self.img_names)\n\n    def __getitem__(self, i):\n        return self._read_img(i)\n\n    def _read_img(self, i):\n        img = Image.open(os.path.join(self.img_dir,self.img_names[i]))\n        return self.transform(img)\n\ndef display_tensor(t):\n    trans = transforms.ToPILImage()\n    display(trans(t))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load evaluation data\neval_data = loadEvaluation(evaluation)\n# Prepare data loader\neval_loader = torch.utils.data.DataLoader(eval_data, batch_size=batch_size, shuffle=True, num_workers=4)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image = train_set[32][0]\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\nimage = image.cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image = eval_data[42]\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\nimage = image.cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out = model_ft(image)\n_, preds = torch.max(out, 1)\npreds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = []\nfor tensor in eval_data:\n    image=tensor.reshape((1, tensor.shape[0], tensor.shape[1], tensor.shape[2]))\n    image = image.cuda()\n    out = model_ft(image)\n    _, preds = torch.max(out, 1)\n    val = preds.cpu().numpy()\n    results.append(val[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"results=np.array(results)\nres=[]\nfor result in results:\n    aux = \" \"+str(result)\n    res.append(aux)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names = np.array(eval_data.img_names)\nnames","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names_f = []\nfor name in names:\n    a=name.replace('.jpg', '')\n    names_f.append(a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names_final = np.array(names_f)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" df = pd.DataFrame({'Id':names_final, 'Category':res})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv(path_or_buf='/kaggle/working/submission_1.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%bash \ncd /kaggle/working/\nless submission.csv","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}